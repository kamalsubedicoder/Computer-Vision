{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afa95ad2",
   "metadata": {},
   "source": [
    "# Computer Vision \n",
    "## Homework 4\n",
    "## Kamal Subedi "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b88524",
   "metadata": {},
   "source": [
    "Q) 5.7) Consider convolving a 256 × 256 × 3 image with 64 separate convolution kernels. For kernels with heights and widths of {(3 × 3), (5 × 5), (7 × 7), and (9 × 9)}, answer each of the following:\n",
    "1. How many parameters (i.e., weights) make up the convolution operation?\n",
    "2. What is the output size after convolving the image with the kernels?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9588db88",
   "metadata": {},
   "source": [
    "The number of parameters in a convolutional layer depends on the kernel size and the number of output channels. In this case, we have 64 separate convolution kernels with different sizes: 3x3, 5x5, 7x7, and 9x9."
   ]
  },
  {
   "cell_type": "raw",
   "id": "490b72aa",
   "metadata": {},
   "source": [
    "The output size of the convolution operation depends on the input image size, kernel size, stride, and padding. However, in the question, the stride or padding haven't been specified. We typically assume a stride of 1 and no padding.\n",
    "\n",
    "Assuming stride = 1 and padding = 0:\n",
    "\n",
    "The output size can be calculated using the following formula:\n",
    "\n",
    "Output Size = (Input Size - Kernel Size + 2 * Padding) / Stride + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a823ded",
   "metadata": {},
   "source": [
    "##### For 3*3 kernel"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4c5a13fc",
   "metadata": {},
   "source": [
    "1. \n",
    "Here, the kernel size is 3*3. For a 3x3 kernel applied to a 3-channel input image, we need 3x3x3+1 (for bais) parameters per kernel.\n",
    "for, 64 kernel with this size, we need 64*(3*3*3+1) = 1792 Parameters. \n",
    "\n",
    "2. \n",
    "For a 256x256x3 input image and 3x3 kernels: Output Size = (256 - 3 + 2*0) / 1 + 1 = (254, 254) and number of filter is 64. So, the output shape of the image is (254,254,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6861798",
   "metadata": {},
   "source": [
    "##### For 5*5 Kernel "
   ]
  },
  {
   "cell_type": "raw",
   "id": "2b143fa1",
   "metadata": {},
   "source": [
    "1. \n",
    "Here, the kernel size is 5*5. \n",
    "for, 64 kernel with this size, we need 64*(5*5*3+1) = 4864 Parameters. \n",
    "\n",
    "2. \n",
    "For a 256x256x3 input image and 5x5 kernels: Output Size = (256 - 5 + 2*0) / 1 + 1 = (252,252,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77051b29",
   "metadata": {},
   "source": [
    "##### For 7*7 Kernel"
   ]
  },
  {
   "cell_type": "raw",
   "id": "db91ccdc",
   "metadata": {},
   "source": [
    "1. \n",
    "Here, the kernel size is 7*7. \n",
    "for, 64 kernel with this size, we need 64*(7*7*3+1) = 9472 Parameters. \n",
    "\n",
    "2. \n",
    "For a 256x256x3 input image and 5x5 kernels: Output Size = (256 - 7 + 2*0) / 1 + 1 = (250,250,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10306273",
   "metadata": {},
   "source": [
    "##### For 9*9 Kernel "
   ]
  },
  {
   "cell_type": "raw",
   "id": "1751018d",
   "metadata": {},
   "source": [
    "1. \n",
    "Here, the kernel size is 9*9.\n",
    "for, 64 kernel with this size, we need 64*(9*9*3+1) = 15616 Parameters. \n",
    "\n",
    "2. \n",
    "For a 256x256x3 input image and 5x5 kernels: Output Size = (256 - 9 + 2*0) / 1 + 1 = (248,248,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123faeae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad12d131",
   "metadata": {},
   "source": [
    "Q) Suppose your model is performing significantly better on the training data than it is on the validation data. What changes might be made to the loss function, training data, and network architecture to prevent such overfitting?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db72f19",
   "metadata": {},
   "source": [
    "When our model is performing significantly better on the training data than on the validation data, it's a clear indication of overfitting. Overfitting occurs when the model learns to fit the training data very closely, including its noise and randomness, which doesn't generalize well to unseen data. To prevent overfitting, We can make several changes to the loss function, training data, and network architecture:\n",
    "\n",
    "1. **Loss Function**:\n",
    "   - Regularization: Add regularization terms to the loss function, such as L1 or L2 regularization. This encourages the model to have smaller weights and makes it less likely to fit noise in the data.\n",
    "   - Dropout: Implement dropout layers or dropout regularization within the network architecture. Dropout randomly deactivates a fraction of neurons during training, preventing the network from relying too heavily on any one neuron.\n",
    "\n",
    "2. **Training Data**:\n",
    "   - More Data: Increasing the size of our training dataset can help the model generalize better. Collecting more diverse and relevant data can reduce overfitting.\n",
    "   - Data Augmentation: Apply data augmentation techniques to artificially increase the diversity of our training data. This can include rotations, translations, flips, and other transformations.\n",
    "\n",
    "3. **Network Architecture**:\n",
    "   - Reduce Model Complexity: Simplify the architecture by reducing the number of layers or neurons. A smaller model is less prone to overfitting.\n",
    "   - Early Stopping: Implement early stopping during training. Monitor the validation loss, and stop training when it starts to increase while the training loss continues to decrease.\n",
    "   - Use Pretrained Models: Transfer learning can be effective. Start with a pretrained model and fine-tune it for your specific task. This leverages knowledge learned from a larger dataset.\n",
    "   - Batch Normalization: Implement batch normalization layers within the network. This can stabilize and speed up training while acting as a regularizer.\n",
    "\n",
    "5. **Hyperparameter Tuning**:\n",
    "   - Experiment with hyperparameters like learning rate, batch size, and optimization algorithms to find the values that work best for your problem.\n",
    "\n",
    "These are the strategies, we may change to effectively mitigate overfitting in our specific machine learning model. It depends on our problem and the nature of data that which strategies should we need to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ec47a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a808ea7",
   "metadata": {},
   "source": [
    "Q) 5.11) Pooling layers and 1 × 1 convolutions are both commonly used to shrink the size of the proceeding layer. When would you use one over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f1262e",
   "metadata": {},
   "source": [
    "Here, \n",
    "\n",
    "Pooling layers are employed for spatial downsampling, reducing feature map dimensions and creating a spatial hierarchy of features. They control model complexity and prevent overfitting by simplifying spatial information.\n",
    "\n",
    "Conversely, 1x1 convolutions reduce the dimensionality within feature maps, perform channel-wise transformations, and introduce non-linearity. They adaptively capture complex feature interactions.\n",
    "\n",
    "The choice between them depends on network goals. Use pooling for spatial downsampling and simplification, while 1x1 convolutions are best for channel-wise dimensionality reduction and feature enhancement. Often, a combination of both is employed to optimize feature extraction and control dimensionality effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f432fe21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
